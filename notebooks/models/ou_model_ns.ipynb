{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over/Under Model (Non-Scaled Features)\n",
    "\n",
    "This notebook provides an analysis pipeline for predicting whether the Over or the Under will hit in a given NFL game. It covers key aspects such as feature selection, model training, and evaluation. This notebook is specifically for non-scaled features, and the models. \n",
    "\n",
    "It applies many different feature selection techniques and trains and evaluates the various models using the different features that were selected from the various feature selection techniques.\n",
    "\n",
    "By following this pipeline, one can gain an insight into the performance of different models based on different feature selection techniques and models on predicting whether the Over/Under will hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif, VarianceThreshold, RFE\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "\n",
    "# Sklearn Model packages\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['spread', 'ou_value', 'fav_ml_result', 'fav_sp_result', 'ou_result',\n",
       "       'fav_ppg', 'und_ppg', 'fav_papg', 'und_papg', 'fav_ypg', 'und_ypg',\n",
       "       'fav_yapg', 'und_yapg', 'fav_topg', 'und_topg', 'fav_tofpg',\n",
       "       'und_tofpg', 'fav_avg_mov', 'und_avg_mov', 'fav_win_pct', 'und_win_pct',\n",
       "       'fav_last_5_win_pct', 'und_last_5_win_pct', 'fav_home_win_pct',\n",
       "       'und_home_win_pct', 'fav_away_win_pct', 'und_away_win_pct', 'ppg_diff',\n",
       "       'ypg_diff', 'topg_diff', 'avg_mov_diff', 'win_pct_diff',\n",
       "       'last_5_win_pct_diff', 'team_ovr_diff', 'ypg_sum', 'ppg_ratio',\n",
       "       'ypg_ratio', 'avg_mov_ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/epainter/Desktop/bet_model_v2/data/processing/fc_3.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My **df** has many features that are redundant or of no use to our model. I only want to work some of the features that I believe to be of some importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ou_result</th>\n",
       "      <th>ou_value</th>\n",
       "      <th>fav_avg_mov</th>\n",
       "      <th>fav_win_pct</th>\n",
       "      <th>und_win_pct</th>\n",
       "      <th>fav_last_5_win_pct</th>\n",
       "      <th>und_last_5_win_pct</th>\n",
       "      <th>ppg_diff</th>\n",
       "      <th>ypg_diff</th>\n",
       "      <th>topg_diff</th>\n",
       "      <th>avg_mov_diff</th>\n",
       "      <th>win_pct_diff</th>\n",
       "      <th>last_5_win_pct_diff</th>\n",
       "      <th>team_ovr_diff</th>\n",
       "      <th>ypg_sum</th>\n",
       "      <th>ppg_ratio</th>\n",
       "      <th>ypg_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>53.5</td>\n",
       "      <td>14.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.35</td>\n",
       "      <td>729.00</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>49.5</td>\n",
       "      <td>-13.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-13.00</td>\n",
       "      <td>123.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-26.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>889.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>32.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.31</td>\n",
       "      <td>687.00</td>\n",
       "      <td>6.33</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>39.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>150.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-10.92</td>\n",
       "      <td>658.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-16.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>760.00</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-17.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>-28.62</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-21.00</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-8.25</td>\n",
       "      <td>684.38</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>0</td>\n",
       "      <td>42.5</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>11.50</td>\n",
       "      <td>86.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>9.33</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>12.55</td>\n",
       "      <td>629.37</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.50</td>\n",
       "      <td>38.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.26</td>\n",
       "      <td>770.18</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>1</td>\n",
       "      <td>47.5</td>\n",
       "      <td>-7.33</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.50</td>\n",
       "      <td>46.25</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>7.50</td>\n",
       "      <td>688.37</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>9.67</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-3.24</td>\n",
       "      <td>-40.88</td>\n",
       "      <td>0.12</td>\n",
       "      <td>11.34</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>777.50</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1071 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ou_result  ou_value  fav_avg_mov  fav_win_pct  und_win_pct  \\\n",
       "0             1      53.5        14.00         1.00         0.00   \n",
       "1             1      49.5       -13.00         0.00         1.00   \n",
       "2             0      47.0        32.00         1.00         0.00   \n",
       "3             1      39.5        10.00         1.00         0.00   \n",
       "4             1      48.0         4.00         1.00         0.00   \n",
       "...         ...       ...          ...          ...          ...   \n",
       "1066          0      35.0       -17.00         0.31         0.62   \n",
       "1067          0      42.5         0.33         0.69         0.31   \n",
       "1068          1      40.0         6.33         0.75         0.56   \n",
       "1069          1      47.5        -7.33         0.69         0.25   \n",
       "1070          0      48.0         9.67         0.62         0.69   \n",
       "\n",
       "      fav_last_5_win_pct  und_last_5_win_pct  ppg_diff  ypg_diff  topg_diff  \\\n",
       "0                    1.0                 0.0     14.00      9.00      -1.00   \n",
       "1                    0.0                 1.0    -13.00    123.00       2.00   \n",
       "2                    1.0                 0.0     32.00     75.00      -2.00   \n",
       "3                    1.0                 0.0     10.00    150.00       0.00   \n",
       "4                    1.0                 0.0      4.00    -16.00       0.00   \n",
       "...                  ...                 ...       ...       ...        ...   \n",
       "1066                 0.2                 0.4     -1.50    -28.62      -0.44   \n",
       "1067                 0.2                 0.4     11.50     86.25       0.38   \n",
       "1068                 0.8                 0.8      5.50     38.94       0.00   \n",
       "1069                 0.6                 0.0      9.50     46.25      -0.87   \n",
       "1070                 0.8                 0.6     -3.24    -40.88       0.12   \n",
       "\n",
       "      avg_mov_diff  win_pct_diff  last_5_win_pct_diff  team_ovr_diff  ypg_sum  \\\n",
       "0            28.00          1.00                  1.0           5.35   729.00   \n",
       "1           -26.00         -1.00                 -1.0          -1.00   889.00   \n",
       "2            64.00          1.00                  1.0           4.31   687.00   \n",
       "3            20.00          1.00                  1.0         -10.92   658.00   \n",
       "4             8.00          1.00                  1.0          -0.89   760.00   \n",
       "...            ...           ...                  ...            ...      ...   \n",
       "1066        -21.00         -0.31                 -0.2          -8.25   684.38   \n",
       "1067          9.33          0.38                 -0.2          12.55   629.37   \n",
       "1068          0.66          0.19                  0.0          14.26   770.18   \n",
       "1069          1.67          0.44                  0.6           7.50   688.37   \n",
       "1070         11.34         -0.07                  0.2          -4.00   777.50   \n",
       "\n",
       "      ppg_ratio  ypg_ratio  \n",
       "0          1.70       1.02  \n",
       "1          0.66       1.32  \n",
       "2          6.33       1.25  \n",
       "3          1.59       1.59  \n",
       "4          1.13       0.96  \n",
       "...         ...        ...  \n",
       "1066       0.93       0.92  \n",
       "1067       1.77       1.32  \n",
       "1068       1.23       1.11  \n",
       "1069       1.48       1.14  \n",
       "1070       0.89       0.90  \n",
       "\n",
       "[1071 rows x 17 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['ou_result',\n",
    "        'ou_value', 'fav_avg_mov', 'fav_win_pct', 'und_win_pct',\n",
    "       'fav_last_5_win_pct', 'und_last_5_win_pct', 'ppg_diff',\n",
    "       'ypg_diff', 'topg_diff', 'avg_mov_diff', 'win_pct_diff',\n",
    "       'last_5_win_pct_diff', 'team_ovr_diff', 'ypg_sum', 'ppg_ratio','ypg_ratio']\n",
    "\n",
    "ou_df = df[cols]\n",
    "\n",
    "ou_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Process\n",
    "\n",
    "In the following cells, I am running 5 different feature importannce techniques. \n",
    "\n",
    "    1. Recursive Feature Elimination\n",
    "    2. Lasso \n",
    "    3. Random Forest\n",
    "    4. Mutual Information\n",
    "    5. ANOVA F-Value\n",
    "\n",
    "For the first 3, I am grouping them together. I have a function *select_features* that applies the 3 feature importance techniques and sorts them in descending order of there importance. \n",
    "\n",
    "For the next 2 (4-5), it selects features based on mutual information and F_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((910, 16), (161, 16), (910,), (161,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = ou_df.drop(columns=['ou_result'])\n",
    "y = ou_df['ou_result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE, Lasso, & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epainter/Desktop/bet_model_v2/venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/epainter/Desktop/bet_model_v2/venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/epainter/Desktop/bet_model_v2/venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/epainter/Desktop/bet_model_v2/venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/epainter/Desktop/bet_model_v2/venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epainter/Desktop/bet_model_v2/venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "def select_features(X, y):\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "    \n",
    "    # 1.) Recursive Feature Elimination\n",
    "    rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=10, step=1)\n",
    "    rfe_selector = rfe_selector.fit(X_train, y_train)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_features = X.loc[:,rfe_support].columns.tolist()\n",
    "    \n",
    "    # 2.) Lasso\n",
    "    lasso = Lasso(alpha=0.1)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    lasso_support = lasso.coef_ != 0\n",
    "    lasso_features = X.loc[:,lasso_support].columns.tolist()\n",
    "    \n",
    "    # 3.) Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_features = X.columns[rf.feature_importances_.argsort()[::-1][:10]].tolist()\n",
    "    \n",
    "    # Combine results\n",
    "    feature_selection_df = pd.DataFrame({'Feature': X.columns,\n",
    "                                         'RFE': rfe_support,\n",
    "                                         'Lasso': lasso_support,\n",
    "                                         'RF': rf.feature_importances_})\n",
    "    \n",
    "    # Count the methods that selected each feature\n",
    "    feature_selection_df['Total'] = np.sum(feature_selection_df.iloc[:, 1:], axis=1)\n",
    "    \n",
    "    # Sort with the most important features (selected by most methods) on top\n",
    "    feature_selection_df = feature_selection_df.sort_values('Total', ascending=False)\n",
    "    \n",
    "    return feature_selection_df\n",
    "\n",
    "# Usage\n",
    "feature_importance = select_features(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fav_avg_mov', 'fav_last_5_win_pct', 'und_last_5_win_pct', 'ppg_diff',\n",
       "       'avg_mov_diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_sel = SelectKBest(mutual_info_classif, k=5)\n",
    "mi_sel.fit(X_train, y_train)\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_feature_indices = mi_sel.get_support(indices=True)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_names = X_train.columns[selected_feature_indices]\n",
    "\n",
    "# Create a new DataFrame with only the selected features\n",
    "X_train_mi = pd.DataFrame(mi_sel.transform(X_train), \n",
    "                                columns=selected_feature_names, \n",
    "                                index=X_train.index)\n",
    "\n",
    "X_test_mi = pd.DataFrame(mi_sel.transform(X_test),\n",
    "                        columns=selected_feature_names,\n",
    "                        index=X_test.index)\n",
    "\n",
    "X_train_mi.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['und_last_5_win_pct', 'ypg_diff', 'topg_diff', 'avg_mov_diff',\n",
       "       'ypg_sum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_sel = SelectKBest(f_classif, k=5)\n",
    "f_sel.fit(X_train, y_train)\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_feature_indices = f_sel.get_support(indices=True)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_names = X_train.columns[selected_feature_indices]\n",
    "\n",
    "# Create a new DataFrame with only the selected features\n",
    "X_train_f = pd.DataFrame(f_sel.transform(X_train), \n",
    "                                columns=selected_feature_names, \n",
    "                                index=X_train.index)\n",
    "\n",
    "X_test_f = pd.DataFrame(f_sel.transform(X_test), \n",
    "                                columns=selected_feature_names, \n",
    "                                index=X_test.index)\n",
    "\n",
    "X_train_f.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Some of this step overlaps with the last step. For Mutual Information and ANOVA F-Value, it has already found the most important features and created a data frame that can be used in our model. \n",
    "\n",
    "For the first 3 methods (RFE, Lasso, RF) we need to select the n (I chose 5) most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort features by 'Total' and select top n\n",
    "top_5_features = feature_importance.sort_values('Total', ascending=False)['Feature'].head(5).tolist()\n",
    "\n",
    "# Create a mask for the top 5 features\n",
    "top_5_mask = X_train.columns.isin(top_5_features)\n",
    "\n",
    "# Selecting only top n features from training and test sets\n",
    "X_train_top5 = X_train.loc[:, top_5_mask]\n",
    "X_test_top5 = X_test.loc[:, top_5_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Here I have defined the models to be used and evaluated. These models are robust to non-scaling of features. \n",
    "\n",
    "THe models include Decision Tress, Random Forest Classifiers, Gradient Boosting, GaussianNB (standard), and some other classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    GaussianNB(),\n",
    "    xgb.XGBClassifier(random_state=42),\n",
    "    lgb.LGBMClassifier(random_state=42)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to evaluate each model for each feature subset of features that were selected based on the different feature selection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE, Lasso, & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DecisionTreeClassifier Results (RFE, Lasso, Random Forest):\n",
      "Accuracy: 0.5403726708074534\n",
      "Confusion Matrix:\n",
      " [[50 43]\n",
      " [31 37]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.54      0.57        93\n",
      "           1       0.46      0.54      0.50        68\n",
      "\n",
      "    accuracy                           0.54       161\n",
      "   macro avg       0.54      0.54      0.54       161\n",
      "weighted avg       0.55      0.54      0.54       161\n",
      "\n",
      "\n",
      "RandomForestClassifier Results (RFE, Lasso, Random Forest):\n",
      "Accuracy: 0.5031055900621118\n",
      "Confusion Matrix:\n",
      " [[54 39]\n",
      " [41 27]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.58      0.57        93\n",
      "           1       0.41      0.40      0.40        68\n",
      "\n",
      "    accuracy                           0.50       161\n",
      "   macro avg       0.49      0.49      0.49       161\n",
      "weighted avg       0.50      0.50      0.50       161\n",
      "\n",
      "\n",
      "GradientBoostingClassifier Results (RFE, Lasso, Random Forest):\n",
      "Accuracy: 0.577639751552795\n",
      "Confusion Matrix:\n",
      " [[66 27]\n",
      " [41 27]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.71      0.66        93\n",
      "           1       0.50      0.40      0.44        68\n",
      "\n",
      "    accuracy                           0.58       161\n",
      "   macro avg       0.56      0.55      0.55       161\n",
      "weighted avg       0.57      0.58      0.57       161\n",
      "\n",
      "\n",
      "GaussianNB Results (RFE, Lasso, Random Forest):\n",
      "Accuracy: 0.5341614906832298\n",
      "Confusion Matrix:\n",
      " [[76 17]\n",
      " [58 10]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.82      0.67        93\n",
      "           1       0.37      0.15      0.21        68\n",
      "\n",
      "    accuracy                           0.53       161\n",
      "   macro avg       0.47      0.48      0.44       161\n",
      "weighted avg       0.48      0.53      0.48       161\n",
      "\n",
      "\n",
      "XGBClassifier Results (RFE, Lasso, Random Forest):\n",
      "Accuracy: 0.5279503105590062\n",
      "Confusion Matrix:\n",
      " [[55 38]\n",
      " [38 30]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.59      0.59        93\n",
      "           1       0.44      0.44      0.44        68\n",
      "\n",
      "    accuracy                           0.53       161\n",
      "   macro avg       0.52      0.52      0.52       161\n",
      "weighted avg       0.53      0.53      0.53       161\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 426, number of negative: 484\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 991\n",
      "[LightGBM] [Info] Number of data points in the train set: 910, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468132 -> initscore=-0.127646\n",
      "[LightGBM] [Info] Start training from score -0.127646\n",
      "\n",
      "LGBMClassifier Results (RFE, Lasso, Random Forest):\n",
      "Accuracy: 0.484472049689441\n",
      "Confusion Matrix:\n",
      " [[49 44]\n",
      " [39 29]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.54        93\n",
      "           1       0.40      0.43      0.41        68\n",
      "\n",
      "    accuracy                           0.48       161\n",
      "   macro avg       0.48      0.48      0.48       161\n",
      "weighted avg       0.49      0.48      0.49       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train_top5, y_train)\n",
    "    \n",
    "    # Predictions using test data\n",
    "    y_pred = model.predict(X_test_top5)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"\\n{model.__class__.__name__} Results (RFE, Lasso, Random Forest):\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DecisionTreeClassifier Results (Mutual Information):\n",
      "Accuracy: 0.484472049689441\n",
      "Confusion Matrix:\n",
      " [[47 46]\n",
      " [37 31]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.51      0.53        93\n",
      "           1       0.40      0.46      0.43        68\n",
      "\n",
      "    accuracy                           0.48       161\n",
      "   macro avg       0.48      0.48      0.48       161\n",
      "weighted avg       0.49      0.48      0.49       161\n",
      "\n",
      "\n",
      "RandomForestClassifier Results (Mutual Information):\n",
      "Accuracy: 0.4968944099378882\n",
      "Confusion Matrix:\n",
      " [[49 44]\n",
      " [37 31]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.53      0.55        93\n",
      "           1       0.41      0.46      0.43        68\n",
      "\n",
      "    accuracy                           0.50       161\n",
      "   macro avg       0.49      0.49      0.49       161\n",
      "weighted avg       0.50      0.50      0.50       161\n",
      "\n",
      "\n",
      "GradientBoostingClassifier Results (Mutual Information):\n",
      "Accuracy: 0.5217391304347826\n",
      "Confusion Matrix:\n",
      " [[58 35]\n",
      " [42 26]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.62      0.60        93\n",
      "           1       0.43      0.38      0.40        68\n",
      "\n",
      "    accuracy                           0.52       161\n",
      "   macro avg       0.50      0.50      0.50       161\n",
      "weighted avg       0.52      0.52      0.52       161\n",
      "\n",
      "\n",
      "GaussianNB Results (Mutual Information):\n",
      "Accuracy: 0.5279503105590062\n",
      "Confusion Matrix:\n",
      " [[75 18]\n",
      " [58 10]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.81      0.66        93\n",
      "           1       0.36      0.15      0.21        68\n",
      "\n",
      "    accuracy                           0.53       161\n",
      "   macro avg       0.46      0.48      0.44       161\n",
      "weighted avg       0.48      0.53      0.47       161\n",
      "\n",
      "\n",
      "XGBClassifier Results (Mutual Information):\n",
      "Accuracy: 0.4968944099378882\n",
      "Confusion Matrix:\n",
      " [[47 46]\n",
      " [35 33]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.51      0.54        93\n",
      "           1       0.42      0.49      0.45        68\n",
      "\n",
      "    accuracy                           0.50       161\n",
      "   macro avg       0.50      0.50      0.49       161\n",
      "weighted avg       0.51      0.50      0.50       161\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 426, number of negative: 484\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 571\n",
      "[LightGBM] [Info] Number of data points in the train set: 910, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468132 -> initscore=-0.127646\n",
      "[LightGBM] [Info] Start training from score -0.127646\n",
      "\n",
      "LGBMClassifier Results (Mutual Information):\n",
      "Accuracy: 0.45962732919254656\n",
      "Confusion Matrix:\n",
      " [[42 51]\n",
      " [36 32]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.45      0.49        93\n",
      "           1       0.39      0.47      0.42        68\n",
      "\n",
      "    accuracy                           0.46       161\n",
      "   macro avg       0.46      0.46      0.46       161\n",
      "weighted avg       0.47      0.46      0.46       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "\n",
    "    model.fit(X_train_mi, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_mi)\n",
    "    \n",
    "    print(f\"\\n{model.__class__.__name__} Results (Mutual Information):\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DecisionTreeClassifier Results (ANOVA):\n",
      "Accuracy: 0.5031055900621118\n",
      "Confusion Matrix:\n",
      " [[49 44]\n",
      " [36 32]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.53      0.55        93\n",
      "           1       0.42      0.47      0.44        68\n",
      "\n",
      "    accuracy                           0.50       161\n",
      "   macro avg       0.50      0.50      0.50       161\n",
      "weighted avg       0.51      0.50      0.51       161\n",
      "\n",
      "\n",
      "RandomForestClassifier Results (ANOVA):\n",
      "Accuracy: 0.5341614906832298\n",
      "Confusion Matrix:\n",
      " [[60 33]\n",
      " [42 26]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.65      0.62        93\n",
      "           1       0.44      0.38      0.41        68\n",
      "\n",
      "    accuracy                           0.53       161\n",
      "   macro avg       0.51      0.51      0.51       161\n",
      "weighted avg       0.53      0.53      0.53       161\n",
      "\n",
      "\n",
      "GradientBoostingClassifier Results (ANOVA):\n",
      "Accuracy: 0.577639751552795\n",
      "Confusion Matrix:\n",
      " [[67 26]\n",
      " [42 26]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.72      0.66        93\n",
      "           1       0.50      0.38      0.43        68\n",
      "\n",
      "    accuracy                           0.58       161\n",
      "   macro avg       0.56      0.55      0.55       161\n",
      "weighted avg       0.57      0.58      0.57       161\n",
      "\n",
      "\n",
      "GaussianNB Results (ANOVA):\n",
      "Accuracy: 0.546583850931677\n",
      "Confusion Matrix:\n",
      " [[79 14]\n",
      " [59  9]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.85      0.68        93\n",
      "           1       0.39      0.13      0.20        68\n",
      "\n",
      "    accuracy                           0.55       161\n",
      "   macro avg       0.48      0.49      0.44       161\n",
      "weighted avg       0.50      0.55      0.48       161\n",
      "\n",
      "\n",
      "XGBClassifier Results (ANOVA):\n",
      "Accuracy: 0.4906832298136646\n",
      "Confusion Matrix:\n",
      " [[51 42]\n",
      " [40 28]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.55      0.55        93\n",
      "           1       0.40      0.41      0.41        68\n",
      "\n",
      "    accuracy                           0.49       161\n",
      "   macro avg       0.48      0.48      0.48       161\n",
      "weighted avg       0.49      0.49      0.49       161\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 426, number of negative: 484\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 890\n",
      "[LightGBM] [Info] Number of data points in the train set: 910, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468132 -> initscore=-0.127646\n",
      "[LightGBM] [Info] Start training from score -0.127646\n",
      "\n",
      "LGBMClassifier Results (ANOVA):\n",
      "Accuracy: 0.5093167701863354\n",
      "Confusion Matrix:\n",
      " [[54 39]\n",
      " [40 28]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.58      0.58        93\n",
      "           1       0.42      0.41      0.41        68\n",
      "\n",
      "    accuracy                           0.51       161\n",
      "   macro avg       0.50      0.50      0.50       161\n",
      "weighted avg       0.51      0.51      0.51       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "\n",
    "    model.fit(X_train_f, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_f)\n",
    "    \n",
    "    print(f\"\\n{model.__class__.__name__} Results (ANOVA):\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "From the models above the DecisionTree using RFE, Laso, and RF feature selection showed promising results with an accuracy of ~54% which is better than the historical ~50% of times that the over hits in a game. \n",
    "\n",
    "In addition the GradientBoosting using ANOVA featrue selection technique was promising with an accuracy of ~57%, however some of the performance metrics were not as strong as they could be, escpecially for the f-1 score and recall, but the precision showed some promise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
